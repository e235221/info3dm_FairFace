### FairFaceとは

- 知能情報総合実験　データマイニング班で扱うデータセット
- ref. https://github.com/dchen236/FairFace

# ディレクトリ構造 
- fair_face_models
  - google driveに共有された事前学習済みモデルをダウンロード￼：
- race_4モデル（White, Black, Asian, Indian）
- race_7モデル（White, Black, Latino_Hispanic, East, Southeast Asian, Indian, Middle Eastern）

ref. https://drive.google.com/drive/folders/1F_pXfbzWvG-bhCpNsRj6F_xsdjpesiFu


## 各ファイルの説明
- predict.py
  - 検出された顔画像に対して、各属性の予測結果（人種、性別、年齢など）とそれに対応するスコアをCSVファイルとして出力します

- predict_bbox.py
	- 検出された顔画像に対して、各属性の予測結果（人種、性別、年齢など）とそれに対応するスコアをCSVファイルとして出力します。（predict.pyと同様。）
	- それに加え，検出された顔のバウンディングボックス情報もCSVファイルに含めます。

test_imgs.csv
推論対象となるテスト画像のファイルパスが記載されたサンプルCSVファイルです。各画像のパスは "img_path" カラムで指定されています。

test_outputs.csv
predict.pyまたはpredict_bbox.pyの実行後に生成される出力結果のCSVファイルです。各顔画像に対する属性の予測結果（および predict_bbox.py の場合はバウンディングボックス情報）がまとめられています。

detected_faces/
dlibの顔検出により、1つの画像から複数の顔が検出された場合に、切り出した各顔画像が保存されるフォルダです。

dlib_models/
dlib の顔検出用の事前学習済みモデル（通常、mmod_human_face_detector.dat やshape_predictor_5_face_landmarks.datなど）が配置されています。

examples/
プロジェクトで利用するためのサンプル画像が含まれており、README.md内のプレビュー画像などで利用されています。

fair_face_models/
事前学習済みのフェイス属性推論モデル（ResNet34ベースなど）の.ptファイルが配置されています。predict.py や predict_bbox.py はこれらのモデルファイルをロードして推論を実施します。

fairface/ および fairface_env/
プロジェクト内で使用するPythonの仮想環境（venv）がセットアップされているディレクトリです。各種シェル（Bash, Fish, PowerShellなど）用の有効化スクリプトや依存ライブラリが含まれます。

test/
プロジェクトの動作確認や検証を目的としたテストコード、テストスクリプトが配置されているフォルダです。

以上のように、FairFaceリポジトリは顔検出および属性推論のためのスクリプト、使用する事前学習済みモデル、データ管理用ファイル、そして各種補助資料やテストコードで構成されています。

#### NoteBookLMにWhitePaperを読み込ませた

主なトピック：
提供された論文は、顔画像データセットに存在する強い人種的バイアスと、それが機械学習モデルの性能および公平性に与える影響を指摘し、この問題に対処するための新規データセットであるFairFaceを提案した研究である [1-3]。
まず、既存の多くの公開顔画像データセットは、Caucasian（白色人種）の顔画像に著しく偏っており、Latinoなどの他の人種が大幅に過小評価されている点を問題視している [1, 2]。このような偏ったデータセットで訓練されたモデルは、非白色人種グループに対する分類精度が一貫せず、顔分析システムの適用範囲を制限することが示唆されている [1]。これは、訓練データのバイアスがそのままモデルのバイアスにつながり、自動化されたシステムの公平性に関する倫理的な懸念を引き起こす [3]。例えば、いくつかの商用コンピュータービジョンシステムは、ジェンダー分類において、男性や明るい肌の顔に対してより高い性能を示すことが報告されており、これは訓練データの偏りに起因する可能性が指摘されている [4]。既存のデータセットは、新聞やWikipediaなどのオンラインメディアから収集されることが多いが、これらのプラットフォームは白色人種がより頻繁に利用しているか、表示される傾向があることが背景にある [5]。
本論文では、この人種バイアス問題を軽減するため、**人種構成のバランスに重点を置いた新規の顔画像データセット「FairFace」**を構築した [5, 6]。FairFaceデータセットは合計108,501枚の画像を含み、White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, Latinoの7つの人種グループでバランスが取られている [6-8]。これらの画像は主にYFCC-100M Flickrデータセットから収集されており、研究目的での自由な共有が可能なCreative Commonsライセンスの画像が使用されている [5, 6, 9]。データ収集においては、ランダムサンプリングから開始し、推定される国別の人口統計に基づいて各国からの画像数を適応的に調整することで、白色人種に偏らないように工夫している [10, 11]。画像には、Amazon Mechanical Turkのワーカーによって人種、ジェンダー、年齢のラベルがアノテーションされており、人種分類は肌の色ではなく、人間のアノテーターによる物理的な外見に基づく判断を採用している [9, 12, 13]。特に、FairFaceは、LatinoおよびMiddle Easternを含み、East AsianとSoutheast Asianを区別した、初めての大規模なin-the-wild顔属性データセットである点を強調している [8, 14-16]。
提案されたデータセットの有効性を評価するため、既存の顔属性データセットおよび新規に収集された画像データセット（Geo-tagged Tweets, Media Photographs, Protest Datasetなど、FairFaceの学習には使用されていない異なるソースからのデータセット）で評価実験を行った [6, 17-20]。ResNet-34モデルを用いた評価の結果、FairFaceデータセットから学習されたモデルは、これらの新規データセットにおいて、既存のデータセットから学習されたモデルよりも大幅に高い一般化精度を示した [21, 22]。重要な点として、提案モデルは、平均的な精度だけでなく、**人種およびジェンダーグループ間での精度も一貫している（バランスが取れている）**ことが明らかになった [21-24]。FairFaceモデルは、ジェンダー分類において、男性と女性の間、および白色人種と非白色人種の間で1%未満の精度格差を達成した [25]。対照的に、他のデータセットから学習されたモデルは、しばしば男性クラスへの強いバイアスや、非白色人種グループでの不正確さを示した [26]。商用のジェンダー分類APIの評価でも、依然として男性を優遇する傾向が確認された [27, 28]。
本論文の主な貢献は以下の3点であるとまとめている [7, 14]：
1.
既存の顔属性データセットおよびそこから学習されたモデルが、より多くの非白色の顔を含む未知のデータに対してうまく一般化しないことを実証した点。
2.
新規のデータセット（FairFace）から学習されたモデルが、平均だけでなく人種グループ間でもより一貫した（バランスの取れた）性能を示すことを示した点。
3.
LatinoおよびMiddle Easternを含み、East AsianとSoutheast Asianを区別した、初めての大規模なin-the-wild顔属性データセットを提供した点。
結論として、FairFaceデータセットは、コンピュータービジョンシステムにおけるバイアスを測定し軽減するための重要なリソースであり、公正なAIシステムの開発を促進することに貢献すると述べている [24, 29, 30]。データセット、コード、モデルは公開されている [21]。


# memo 
# 機械学習の進め方

- データセット構築　...fair faceを使う
  - データ前処理は，データセット使うから問題ない。
- メインの部分：モデルの選定・構築
  - 分類問題のモデル
- モデルの学習
- 評価
